{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('dates.csv', sep=';', header=None)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.loc[len(data)] = [\"od 21 lutego aż do dnia 29 lutego\"]\n",
    "data.loc[len(data)] = [\"21 luty 22 luty 23 luty\"]\n",
    "data.loc[len(data)] = [\"od dnia 21 lutego do 29 lutego\"]\n",
    "data.loc[len(data)] = [\"lipca i sierpnia rok 2025\"]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    doc = nlp(data.iloc[i][0])\n",
    "    print(\"--- \"+doc.text)\n",
    "    for tok in doc:\n",
    "        print(tok.text, tok.pos_, tok.ent_type_)\n",
    "    for ent in doc.ents:\n",
    "        print(\"++ \", ent.text, \"+++\", ent.start_char, ent.end_char, ent.label_, ent.start)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "patterns = [[\n",
    "        {\"POS\": {\"IN\": [\"ADJ\", \"NUM\"]},  \"OP\": \"?\"},\n",
    "        {\"POS\": \"NOUN\", \"ENT_TYPE\": \"date\", \"LOWER\": {\"REGEX\": \"(?:stycz|lut|mar|kwie|maj|czerw|lip|sierp|wrze|październik|listopad|grud)[a-z]*\"}},\n",
    "        {\"LOWER\": {\"IN\": [\"-\", \"do\", \"i\"]}},\n",
    "        {\"POS\": {\"IN\": [\"ADJ\", \"NUM\"]}, \"OP\": \"?\"},\n",
    "        {\"POS\": \"NOUN\", \"LOWER\": {\"REGEX\": \"(?:stycz|lut|mar|kwie|maj|czerw|lip|sierp|wrze|październik|listopad|grud)[a-z]*\"}}\n",
    "], [\n",
    "        {\"POS\": {\"IN\": [\"ADJ\", \"NUM\"]}, \"ENT_TYPE\": \"date\"},\n",
    "        {\"POS\": \"NOUN\",  \"OP\": \"?\", \"LOWER\": {\"REGEX\": \"(?:stycz|lut|mar|kwie|maj|czerw|lip|sierp|wrze|październik|listopad|grud)[a-z]*\"}},\n",
    "        {\"LOWER\": {\"IN\": [\"-\", \"do\", \"i\"]}},\n",
    "        {\"POS\": {\"IN\": [\"ADJ\", \"NUM\"]}},\n",
    "        {\"POS\": \"NOUN\",  \"OP\": \"?\", \"LOWER\": {\"REGEX\": \"(?:stycz|lut|mar|kwie|maj|czerw|lip|sierp|wrze|październik|listopad|grud)[a-z]*\"}}\n",
    "]]\n",
    "\n",
    "date_patterns = [[\n",
    "    {\"ENT_TYPE\": \"date\", \"OP\": \"?\"},\n",
    "    {\"POS\": \"NOUN\", \"ENT_TYPE\": \"date\", \"LOWER\": {\"REGEX\": \"(?:stycz|lut|mar|kwie|maj|czerw|lip|sierp|wrze|październik|listopad|grud)[a-z]*\"}}\n",
    "]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DatePhrase\", date_patterns)\n",
    "for i in range(len(data)):\n",
    "    doc = nlp(data.iloc[i][0])\n",
    "    print(\"--- \"+doc.text)\n",
    "    matches = matcher(doc)\n",
    "    for m_id, start, end in matches:\n",
    "        print(doc[start:end])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DatePhrase\", patterns)\n",
    "for i in range(len(data)):\n",
    "    doc = nlp(data.iloc[i][0])\n",
    "    print(\"--- \"+doc.text)\n",
    "    matches = matcher(doc)\n",
    "    for m_id, start, end in matches:\n",
    "        print(doc[start:end])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "print(enc)\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allegro/herbert-base-cased')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import XLMTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = XLMTokenizer.from_pretrained('allegro/herbert-klej-cased-tokenizer-v1')\n",
    "enc = tokenizer(data.iloc[0][1])\n",
    "for i in enc['input_ids']:\n",
    "    print(tokenizer.decode(i))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from creating_trip.point_of_interest.poi_from_osm_selectors import selectors\n",
    "from OSMPythonTools.overpass import Overpass\n",
    "from OSMPythonTools.nominatim import Nominatim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "overpass = Overpass()\n",
    "nominatim = Nominatim()\n",
    "region = nominatim.query(\"Europa, Italy\")\n",
    "\n",
    "region_data = region.toJSON()\n",
    "if isinstance(region_data, list):\n",
    "    region_data = region_data[0]\n",
    "\n",
    "region_name = region_data.get(\"name\")\n",
    "print(region_data)\n",
    "float(region_data.get(\"lat\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_str = f'area[\"name\"=\"{region_name}\"]->.searchArea;('\n",
    "for selector in selectors:\n",
    "    query_str += f'nwr[\"{selector[0]}\"=\"{selector[1]}\"](area.searchArea);'\n",
    "query_str += ');out body;>;out skel;'\n",
    "res = overpass.query(query_str)\n",
    "res.toJSON()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from creating_trip.point_of_interest.poi_from_osm_selectors import selectors\n",
    "from OSMPythonTools.overpass import Overpass\n",
    "from OSMPythonTools.nominatim import Nominatim\n",
    "from models.mongo_utils import MongoUtils\n",
    "from creating_trip.point_of_interest.mappings_for_OSM import determine_kinds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_pois(region: str, country: str):\n",
    "    overpass = Overpass()\n",
    "    nominatim = Nominatim()\n",
    "    region_data = nominatim.query(region)\n",
    "\n",
    "    region_data = region_data.toJSON()\n",
    "    if isinstance(region_data, list):\n",
    "        region_data = region_data[0]\n",
    "\n",
    "    region_name = region_data.get(\"name\")\n",
    "    query_str = f'area[\"name\"=\"{region_name}\"]->.searchArea;('\n",
    "    for selector in selectors:\n",
    "        query_str += f'nwr[\"{selector[0]}\"=\"{selector[1]}\"](area.searchArea);'\n",
    "    query_str += ');out body;>;out skel;'\n",
    "    res = overpass.query(query_str)\n",
    "\n",
    "    db_connection = MongoUtils()\n",
    "    collection = db_connection.get_collection_attractions(f\"{country.lower()}-{region.lower()}\")\n",
    "    for element in res.toJSON().get(\"elements\"):\n",
    "        tags = element.get(\"tags\")\n",
    "        if tags is None:\n",
    "            continue\n",
    "        if tags.get(\"name\") is None or element.get(\"type\") is None or element.get(\"id\") is None:\n",
    "            continue\n",
    "\n",
    "        lon = element.get(\"lon\")\n",
    "        lat = element.get(\"lat\")\n",
    "        if lon is None or lat is None:\n",
    "            data = nominatim.query(f\"{element.get('type')}/{element.get('id')}\", lookup=True).toJSON()\n",
    "            if isinstance(data, list):\n",
    "                if len(data) == 0:\n",
    "                    continue\n",
    "                data = data[0]\n",
    "            lon = data.get(\"lon\")\n",
    "            lat = data.get(\"lat\")\n",
    "            if lon is None or lat is None:\n",
    "                continue\n",
    "        lon = float(lon)\n",
    "        lat = float(lat)\n",
    "\n",
    "        kinds = determine_kinds(tags)\n",
    "        if len(kinds) == 0:\n",
    "            continue\n",
    "        collection.insert_one({\n",
    "            \"xid\": f\"{element.get('type')[0].upper()}{element.get('id')}\",\n",
    "            \"name\": tags.get(\"name\"),\n",
    "            \"osm\": f\"{element.get('type')}/{element.get('id')}\",\n",
    "            \"wikidata\": tags.get(\"wikidata\"),\n",
    "            \"wikipedia\": tags.get(\"wikipedia\"),\n",
    "            \"point\":{\n",
    "                \"lon\": lon,\n",
    "                \"lat\": lat\n",
    "            },\n",
    "            \"opening_hours\": tags.get(\"opening_hours\"),\n",
    "            \"url\": tags.get(\"website\"),\n",
    "            \"kinds\": kinds\n",
    "        })\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_pois(\"Gdańsk\", \"Poland\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from humanized_opening_hours import OHParser\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "day_regex = re.compile(\"^(\\w+):\\s*(?:(\\d\\d:\\d\\d)\\s*-\\s*(\\d\\d:\\d\\d)|(closed))$\")\n",
    "\n",
    "opening_hours = \"We-Sa 09:30-15:00; Tu 12:00-18:30\"\n",
    "parser = OHParser(opening_hours)\n",
    "description = parser.render().plaintext_week_description()\n",
    "week = description.split(\"\\n\")\n",
    "print(week)\n",
    "for day in week:\n",
    "    match = day_regex.match(day)\n",
    "    print(match.group(2))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
